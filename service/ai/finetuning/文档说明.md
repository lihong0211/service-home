# 14-Fine-tuning实操 代码生成文档说明

本目录为 **Qwen 系列模型微调实操** 示例，基于 **Unsloth** 框架，涵盖 **纯文本 SFT**、**视觉-语言 VL 微调** 与 **GRPO（R1 推理）微调**。配套数据格式与脚本用法见下文。

---

## 一、目录与文件概览

| 文件 | 类型 | 说明 |
|------|------|------|
| `qwen_vl_car_insurance_train.py` | 视觉-语言微调 | Qwen2.5-VL-3B，汽车保险承保（图片+文本） |
| `qwen-vl-train.xlsx` | 训练数据 | VL 训练用 Excel：id / prompt / image / response |
| `Qwen2_5_(7B)_医疗微调.py` | 纯文本 SFT | Qwen2.5-7B，医疗对话微调 |
| `Qwen2_5_(7B)_Alpaca-2.py` | 纯文本 SFT | Qwen2.5-7B，Alpaca 指令格式微调 |
| `Qwen2_5_(7B)_R1.py` | GRPO/R1 微调 | Qwen2.5-7B，GSM8K 推理+奖励优化 |

---

## 二、qwen_vl_car_insurance_train.py（视觉-语言微调）

### 2.1 功能

- **模型**：Qwen2.5-VL-3B-Instruct（可选 4bit 量化）
- **场景**：汽车保险承保专家——根据车辆里程表等图片抽取信息并回答
- **框架**：Unsloth `FastVisionModel` + LoRA + TRL `SFTTrainer`

### 2.2 数据：qwen-vl-train.xlsx

- **格式**：Excel，列包括  
  - `id`：样本 ID  
  - `prompt`：用户文本指令（如「请从图中提取关键信息」）  
  - `image`：图片路径（脚本会按路径加载本地图片）  
  - `response`：期望的助手回复（标准答案）
- **转换**：脚本内 `convert_excel_to_training_format()` 将每行转为多模态对话格式：  
  `user`: 一条文本 + 一张图片 → `assistant`: 一条文本。

### 2.3 主要流程

1. **加载模型与 LoRA**  
   - `FastVisionModel.from_pretrained(...)`，可选 `use_gradient_checkpointing="unsloth"`。  
   - `get_peft_model(..., finetune_vision_layers=True, finetune_language_layers=True, r=16, lora_alpha=16, ...)`。

2. **读 Excel 并转训练集**  
   - `load_excel_dataset("qwen-vl-train.xlsx")` → DataFrame。  
   - `convert_excel_to_training_format(df)` → `converted_dataset`（messages 列表）。

3. **训练**  
   - `UnslothVisionDataCollator` + `SFTTrainer`，`SFTConfig` 中设置  
     `remove_unused_columns=False`、`dataset_text_field=""`、`dataset_kwargs={"skip_prepare_dataset": True}` 等以适配视觉输入。  
   - 示例：`max_steps=30`、`per_device_train_batch_size=2`、`gradient_accumulation_steps=4`。

4. **推理与保存**  
   - 训练前后均可对 `images/1-vehicle-odometer-reading.jpg` 等做推理测试。  
   - 保存：`model.save_pretrained("car_insurance_lora_model")`，`tokenizer.save_pretrained(...)`。

### 2.4 依赖与路径

- 依赖：`unsloth`、`torch`、`PIL`、`pandas`、`trl` 等。  
- 模型路径在脚本中为 `/root/autodl-tmp/models/Qwen/Qwen2.5-VL-3B-Instruct`，可按环境修改。  
- 测试图片路径：`images/1-vehicle-odometer-reading.jpg`。

---

## 三、Qwen2_5_(7B)_医疗微调.py（纯文本 SFT）

### 3.1 功能

- **模型**：Qwen2.5-7B-Instruct（4bit + LoRA）
- **场景**：医疗问答——根据患者问题生成专业回答
- **数据**：本地目录 `Data_数据` 下多科室 CSV（内科、外科、儿科、肿瘤科、妇产科、男科等）

### 3.2 数据格式

- **目录结构**：`Data_数据/{IM_内科,Surgical_外科,...}/*.csv`。  
- **CSV 列**：支持 `question/问题/ask`、`answer/回答/response` 等列名，脚本会做编码尝试（gbk、utf-8 等）与长度过滤（如 question/answer 各 ≤200 字）。

### 3.3 提示与训练

- **提示模板**：`medical_prompt`，形如「你是一个专业的医疗助手… ### 问题：{} ### 回答：{}」，末尾加 `EOS_TOKEN`。  
- **训练**：`SFTTrainer` + `TrainingArguments`（如 `num_train_epochs=3`、`max_steps=-1`、`dataset_text_field="text"`）。  
- **保存**：`lora_model_medical`；推理示例演示了「仅 LoRA」「基座 + adapter」两种加载方式。

---

## 四、Qwen2_5_(7B)_Alpaca-2.py（Alpaca 指令微调）

### 4.1 功能

- **模型**：Qwen2.5-7B-Instruct（4bit + LoRA）
- **场景**：通用指令跟随，Alpaca 格式
- **数据**：`/root/autodl-tmp/datasets/yahma/alpaca-cleaned`（HuggingFace `datasets` 格式）

### 4.2 数据与格式

- **Alpaca 模板**：Instruction / Input / Response 三段式，脚本中为 `alpaca_prompt`，末尾加 `EOS_TOKEN`。  
- **字段**：`instruction`、`input`、`output`，经 `formatting_prompts_func` 拼成一条 `text` 供 SFT。

### 4.3 训练与导出

- **训练**：`SFTTrainer`，`max_steps=60` 等，与医疗脚本类似。  
- **保存**：`lora_model`。  
- **扩展**：脚本内包含合并为 float16/4bit、GGUF（q8_0、q4_k_m、q5_k_m 等）、推送 HuggingFace 的示例（多为 `if False` 关闭）。

---

## 五、Qwen2_5_(7B)_R1.py（GRPO / R1 推理微调）

### 5.1 功能

- **模型**：Qwen2.5-7B-Instruct（4bit + LoRA，支持 vLLM 快速推理）
- **方法**：GRPO（Generative Reward-Paired Optimization），配合多类奖励做「推理链 + 答案」优化（R1 风格）
- **数据**：GSM8K 数学推理（`/root/autodl-tmp/datasets/gsm8k`），系统提示要求输出 `<reasoning>...</reasoning><answer>...</answer>`

### 5.2 数据与格式

- **输入**：`get_gsm8k_questions()` 将 `question`、`answer`（带 `####` 标准答案）转为带 system/user 的 `prompt` 与用于奖励的 `answer`。  
- **输出格式**：模型需生成 XML：`<reasoning>...</reasoning><answer>...</answer>`，奖励函数会解析并打分。

### 5.3 奖励函数（多目标）

- `correctness_reward_func`：提取 `<answer>` 与标准答案比较，正确 2.0 否则 0。  
- `int_reward_func`：答案为整数给 0.5。  
- `strict_format_reward_func` / `soft_format_reward_func`：正则检查 XML 格式，给 0.5。  
- `xmlcount_reward_func`：基于 XML 标签完整性的细粒度得分。  

训练时 `GRPOTrainer` 的 `reward_funcs` 列表即上述函数组合。

### 5.4 训练与推理

- **配置**：`GRPOConfig`（如 `max_steps=250`、`num_generations=6`、`max_prompt_length=256` 等）。  
- **保存**：`model.save_lora("grpo_saved_lora")`。  
- **推理**：通过 vLLM `fast_generate` + `load_lora("grpo_saved_lora")` 做测试；脚本末尾含合并/GGUF 导出示例（多为 `if False`）。

---

## 六、对比小结

| 脚本 | 模型 | 任务类型 | 数据来源 | 训练器 | 输出 |
|------|------|----------|----------|--------|------|
| qwen_vl_car_insurance_train | Qwen2.5-VL-3B | 视觉+文本 SFT | qwen-vl-train.xlsx | SFTTrainer + VisionDataCollator | car_insurance_lora_model |
| Qwen2_5_(7B)_医疗微调 | Qwen2.5-7B | 文本 SFT | Data_数据/*.csv | SFTTrainer | lora_model_medical |
| Qwen2_5_(7B)_Alpaca-2 | Qwen2.5-7B | 文本 SFT | alpaca-cleaned | SFTTrainer | lora_model |
| Qwen2_5_(7B)_R1 | Qwen2.5-7B | GRPO 推理优化 | GSM8K | GRPOTrainer | grpo_saved_lora |

---

## 七、环境与路径说明

- **共同依赖**：`unsloth`、`torch`、`transformers`、`trl`、`datasets` 等；VL 脚本还需 `PIL`、`pandas`（读 Excel）。  
- **路径**：脚本内模型与数据集路径多为 `/root/autodl-tmp/...`，本地或 Colab 使用需改为本机路径。  
- **显存**：7B 使用 4bit + LoRA 可在 T4 等单卡运行；VL 3B 可根据显存选择是否 `load_in_4bit` 与梯度检查点。

以上为当前目录下各脚本与 `qwen-vl-train.xlsx` 的文档说明，便于复现与二次修改。
