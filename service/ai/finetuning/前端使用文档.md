# 微调模型聊天接口 - 前端使用文档

后端提供 **POST `/ai/finetuning/chat`**，与 `/ai/chat` 请求格式兼容，用于调用已加载的微调模型（如 Qwen2.5-1.5B/7B + 医疗 LoRA）进行对话。支持非流式、流式，以及「基座 vs LoRA」对比模式。

---

## 1. 接口说明

| 项目 | 说明 |
|------|------|
| **URL** | `POST /ai/finetuning/chat` |
| **Content-Type** | `application/json` |
| **请求体** | 见下方 |

---

## 2. 请求体

```ts
// 类型示意（前端可自建 interface）
{
  messages: Array<{ role: "user" | "assistant" | "system"; content: string }>;  // 必填
  stream?: boolean;   // 是否流式，默认 false
  compare?: boolean; // 是否同时返回基座与 LoRA 两路（流式时按 source 区分），默认 false
  options?: {        // 生成参数，可选
    temperature?: number;   // 默认 0.7
    num_predict?: number;   // 最大生成长度，默认 512（同 max_new_tokens）
    top_p?: number;         // 默认 0.9
    repeat_penalty?: number; // 默认 1.1
  };
}
```

- **messages**：对话历史，与 OpenAI 格式一致。`content` 为字符串。
- **stream**：`true` 时使用 SSE 流式返回；`false` 时一次返回完整回复。
- **compare**：`true` 时同时生成「基座模型」与「LoRA 模型」的回复，便于对比。流式时每条事件带 `source: "base" | "lora"`。
- **options**：可选，不传则使用默认生成参数。

---

## 3. 响应格式

### 3.1 非流式（stream: false）

**成功：**

```json
{
  "code": 0,
  "message": {
    "role": "assistant",
    "content": "LoRA 模型的完整回复内容",
    "base_content": "基座模型的完整回复",
    "lora_content": "LoRA 模型的完整回复（与 content 一致）"
  },
  "base": "基座模型的完整回复",
  "lora": "LoRA 模型的完整回复"
}
```

前端展示时可直接用 `message.content` 或 `lora` 作为助手回复；若做对比展示，可用 `base` 与 `lora`。

**错误：**

- `400`：缺少或非法 `messages`、请求体非 JSON 等。
- `404`：Base 模型或 LoRA 目录不存在。
- `500`：服务端异常。

响应体示例：`{ "code": 400, "msg": "Missing or invalid messages" }`。

---

### 3.2 流式（stream: true）

- **Content-Type**：`text/event-stream; charset=utf-8`
- **数据格式**：每行一条 SSE 事件，格式为 `data: <JSON>\n\n`。

#### 普通流式（compare: false）

每条事件示例：

```json
{ "message": { "content": "本段文本" }, "response": "本段文本" }
```

流结束标志：

```
data: [DONE]
```

#### 对比流式（compare: true）

每条事件带 `source`，表示来自基座还是 LoRA：

```json
{ "source": "base", "content": "本段文本" }
```
```json
{ "source": "lora", "content": "本段文本" }
```

某一路结束时发送：

```json
{ "source": "base", "done": true }
```
或
```json
{ "source": "lora", "done": true }
```

最后仍以 `data: [DONE]` 结束。

**错误时**（流中可能出现）：

```json
{ "error": "错误信息", "done": true }
```

---

## 4. 前端调用示例

### 4.1 非流式

```javascript
const res = await fetch("/ai/finetuning/chat", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    messages: [{ role: "user", content: "感冒发烧应该吃什么药？" }],
    stream: false,
    options: { temperature: 0.7, num_predict: 512 },
  }),
});
const data = await res.json();
if (data.code === 0) {
  console.log("助手回复:", data.message.content);
  // 对比时可用 data.base / data.lora
} else {
  console.error(data.msg);
}
```

### 4.2 流式（普通）

```javascript
const res = await fetch("/ai/finetuning/chat", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    messages: [{ role: "user", content: "感冒发烧应该吃什么药？" }],
    stream: true,
    options: { num_predict: 512 },
  }),
});

const reader = res.body.getReader();
const decoder = new TextDecoder();
let content = "";
for (;;) {
  const { value, done } = await reader.read();
  if (done) break;
  const chunk = decoder.decode(value, { stream: true });
  for (const line of chunk.split("\n")) {
    if (!line.startsWith("data: ")) continue;
    const raw = line.slice(6);
    if (raw === "[DONE]") break;
    try {
      const obj = JSON.parse(raw);
      if (obj.response) content += obj.response;
      if (obj.error) console.error(obj.error);
    } catch (_) {}
  }
}
console.log(content);
```

### 4.3 流式（对比 base / lora）

```javascript
const res = await fetch("/ai/finetuning/chat", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    messages: [{ role: "user", content: "头晕怎么办？" }],
    stream: true,
    compare: true,
  }),
});

const reader = res.body.getReader();
const decoder = new TextDecoder();
const contents = { base: "", lora: "" };
for (;;) {
  const { value, done } = await reader.read();
  if (done) break;
  const chunk = decoder.decode(value, { stream: true });
  for (const line of chunk.split("\n")) {
    if (!line.startsWith("data: ")) continue;
    const raw = line.slice(6);
    if (raw === "[DONE]") break;
    try {
      const obj = JSON.parse(raw);
      if (obj.done && obj.source) {
        // 该路结束
      } else if (obj.content !== undefined && obj.source) {
        contents[obj.source] += obj.content;
      }
    } catch (_) {}
  }
}
console.log("基座:", contents.base);
console.log("LoRA:", contents.lora);
```

---

## 5. 注意事项

- 首次请求会触发模型加载，可能需数十秒，后续请求复用已加载模型。
- 多轮对话：将历史与当前用户输入一起放入 `messages` 即可。
- 若需指定 Base 模型或 LoRA 路径，由服务端环境变量 `FINETUNING_BASE_MODEL`、`FINETUNING_LORA_PATH`、`FINETUNING_MODEL_NAME` 控制，前端无需传参。
