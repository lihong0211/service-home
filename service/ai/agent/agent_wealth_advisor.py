"""
æ··åˆå‹æ™ºèƒ½ä½“ - è´¢å¯Œç®¡ç†æŠ•é¡¾AIåŠ©æ‰‹

åŸºäº LangGraph å®ç°çš„æ··åˆå‹æ™ºèƒ½ä½“ï¼Œç»“åˆååº”å¼æ¶æ„çš„å³æ—¶å“åº”èƒ½åŠ›å’Œæ·±æ€ç†Ÿè™‘æ¶æ„çš„é•¿æœŸè§„åˆ’èƒ½åŠ›ï¼Œ
é€šè¿‡åè°ƒå±‚åŠ¨æ€åˆ‡æ¢å¤„ç†æ¨¡å¼ï¼Œæä¾›æ™ºèƒ½åŒ–è´¢å¯Œç®¡ç†å’¨è¯¢æœåŠ¡ã€‚

ä¸‰å±‚æ¶æ„ï¼š
1. åº•å±‚ï¼ˆååº”å¼ï¼‰ï¼šå³æ—¶å“åº”å®¢æˆ·æŸ¥è¯¢ï¼Œæä¾›å¿«é€Ÿåé¦ˆ
2. ä¸­å±‚ï¼ˆåè°ƒï¼‰ï¼šè¯„ä¼°ä»»åŠ¡ç±»å‹å’Œä¼˜å…ˆçº§ï¼ŒåŠ¨æ€é€‰æ‹©å¤„ç†æ¨¡å¼
3. é¡¶å±‚ï¼ˆæ·±æ€ç†Ÿè™‘ï¼‰ï¼šè¿›è¡Œå¤æ‚çš„æŠ•èµ„åˆ†æå’Œé•¿æœŸè´¢åŠ¡è§„åˆ’
"""

import json
import os
from typing import Any, Dict, Literal, Optional, TypedDict

from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph

from service.ai.langchain import graph_to_schema, run_graph_stream_and_collect

_API_KEY = os.environ.get("DASHSCOPE_API_KEY")
_LLM = ChatOpenAI(
    model="qwen-turbo",
    openai_api_key=_API_KEY,
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
    max_tokens=2000,
)


class WealthAdvisorState(TypedDict):
    user_query: str
    customer_profile: Optional[Dict[str, Any]]
    query_type: Optional[Literal["emergency", "informational", "analytical"]]
    processing_mode: Optional[Literal["reactive", "deliberative"]]
    market_data: Optional[Dict[str, Any]]
    analysis_results: Optional[Dict[str, Any]]
    final_response: Optional[str]
    current_phase: Optional[str]
    error: Optional[str]


_ASSESS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªè´¢å¯Œç®¡ç†æŠ•é¡¾AIåŠ©æ‰‹çš„åè°ƒå±‚ã€‚è¯„ä¼°ç”¨æˆ·æŸ¥è¯¢ï¼Œç¡®å®šç±»å‹å’Œå¤„ç†æ¨¡å¼ã€‚

ç”¨æˆ·æŸ¥è¯¢: {user_query}

è¯·ä»¥JSONè¿”å›ï¼š
- query_type: "emergency"|"informational"|"analytical"
- processing_mode: "reactive"|"deliberative"
- reasoning: å†³ç­–ç†ç”±
"""

_REACTIVE_PROMPT = """ä½ æ˜¯è´¢å¯Œç®¡ç†æŠ•é¡¾AIåŠ©æ‰‹ï¼Œæä¾›å¿«é€Ÿå‡†ç¡®çš„å“åº”ã€‚

ç”¨æˆ·æŸ¥è¯¢: {user_query}
å®¢æˆ·ä¿¡æ¯: {customer_profile}

è¯·ç›´æ¥ã€ç®€æ´åœ°å›ç­”ã€‚"""

_DATA_PROMPT = """ä½ æ˜¯æ•°æ®æ”¶é›†æ¨¡å—ã€‚ç”¨æˆ·æŸ¥è¯¢: {user_query}ï¼Œå®¢æˆ·ä¿¡æ¯: {customer_profile}
è¯·æ”¶é›†æ·±å…¥åˆ†ææ‰€éœ€çš„å¸‚åœºå’Œè´¢åŠ¡æ•°æ®ï¼Œä»¥JSONè¿”å›ï¼šrequired_data_types, collected_dataï¼ˆå¯æ¨¡æ‹Ÿï¼‰ã€‚"""

_ANALYSIS_PROMPT = """ä½ æ˜¯åˆ†æå¼•æ“ã€‚ç”¨æˆ·æŸ¥è¯¢: {user_query}ï¼Œå®¢æˆ·ä¿¡æ¯: {customer_profile}ï¼Œå¸‚åœºæ•°æ®: {market_data}
è¯·æä¾›å…¨é¢çš„æŠ•èµ„åˆ†æï¼ˆJSONï¼‰ï¼šå¸‚åœºçŠ¶å†µã€ç»„åˆåˆ†æã€ä¸ªæ€§åŒ–å»ºè®®ã€é£é™©è¯„ä¼°ã€é¢„æœŸå›æŠ¥ã€‚"""

_RECOMMEND_PROMPT = """ä½ æ˜¯è´¢å¯Œç®¡ç†æŠ•é¡¾ã€‚æ ¹æ®åˆ†æç»“æœä¸ºå®¢æˆ·å‡†å¤‡æœ€ç»ˆå»ºè®®ã€‚

ç”¨æˆ·æŸ¥è¯¢: {user_query}
å®¢æˆ·ä¿¡æ¯: {customer_profile}
åˆ†æç»“æœ: {analysis_results}

è¯·æä¾›è‡ªç„¶è¯­è¨€çš„æŠ•èµ„å»ºè®®ï¼šæ€»ä½“ç­–ç•¥ã€å…·ä½“æ­¥éª¤ã€èµ„äº§é…ç½®ã€é£é™©ç®¡ç†ã€æ—¶é—´æ¡†æ¶ã€é¢„æœŸæ”¶ç›Šã€‚"""


def _assess_query(state: WealthAdvisorState) -> WealthAdvisorState:
    try:
        chain = ChatPromptTemplate.from_template(_ASSESS_PROMPT) | _LLM | JsonOutputParser()
        result = chain.invoke({"user_query": state["user_query"]})
        mode = result.get("processing_mode") or "reactive"
        if mode not in ("reactive", "deliberative"):
            mode = "reactive"
        qtype = result.get("query_type") or "emergency"
        if qtype not in ("emergency", "informational", "analytical"):
            qtype = "emergency"
        return {**state, "query_type": qtype, "processing_mode": mode}
    except Exception as e:
        return {**state, "error": str(e), "final_response": "è¯„ä¼°æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯ã€‚"}


def _reactive_processing(state: WealthAdvisorState) -> WealthAdvisorState:
    try:
        chain = ChatPromptTemplate.from_template(_REACTIVE_PROMPT) | _LLM | StrOutputParser()
        resp = chain.invoke({
            "user_query": state["user_query"],
            "customer_profile": json.dumps(state.get("customer_profile") or {}, ensure_ascii=False),
        })
        return {**state, "final_response": resp}
    except Exception as e:
        return {**state, "error": str(e), "final_response": "å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯ã€‚"}


def _collect_data(state: WealthAdvisorState) -> WealthAdvisorState:
    try:
        chain = ChatPromptTemplate.from_template(_DATA_PROMPT) | _LLM | JsonOutputParser()
        result = chain.invoke({
            "user_query": state["user_query"],
            "customer_profile": json.dumps(state.get("customer_profile") or {}, ensure_ascii=False, indent=2),
        })
        return {**state, "market_data": result.get("collected_data", {}), "current_phase": "analyze"}
    except Exception as e:
        return {**state, "error": str(e)}


def _analyze_data(state: WealthAdvisorState) -> WealthAdvisorState:
    if not state.get("market_data"):
        return {**state, "error": "åˆ†æé˜¶æ®µç¼ºå°‘å¸‚åœºæ•°æ®"}
    try:
        chain = ChatPromptTemplate.from_template(_ANALYSIS_PROMPT) | _LLM | JsonOutputParser()
        result = chain.invoke({
            "user_query": state["user_query"],
            "customer_profile": json.dumps(state.get("customer_profile") or {}, ensure_ascii=False, indent=2),
            "market_data": json.dumps(state.get("market_data") or {}, ensure_ascii=False, indent=2),
        })
        return {**state, "analysis_results": result, "current_phase": "recommend"}
    except Exception as e:
        return {**state, "error": str(e)}


def _generate_recommendations(state: WealthAdvisorState) -> WealthAdvisorState:
    if not state.get("analysis_results"):
        return {**state, "error": "å»ºè®®ç”Ÿæˆé˜¶æ®µç¼ºå°‘åˆ†æç»“æœ"}
    try:
        chain = ChatPromptTemplate.from_template(_RECOMMEND_PROMPT) | _LLM | StrOutputParser()
        result = chain.invoke({
            "user_query": state["user_query"],
            "customer_profile": json.dumps(state.get("customer_profile") or {}, ensure_ascii=False, indent=2),
            "analysis_results": json.dumps(state.get("analysis_results") or {}, ensure_ascii=False, indent=2),
        })
        return {**state, "final_response": result, "current_phase": "respond"}
    except Exception as e:
        return {**state, "error": str(e)}


def _respond(state: WealthAdvisorState) -> WealthAdvisorState:
    if not state.get("final_response"):
        return {**state, "final_response": "æ— æ³•ç”Ÿæˆå“åº”ï¼Œè¯·ç¨åé‡è¯•ã€‚", "error": state.get("error", "æœªçŸ¥é”™è¯¯")}
    return state


# èŠ‚ç‚¹ id -> å‰ç«¯å±•ç¤ºï¼ˆä¾› LangGraph å›¾è°±å¯è§†åŒ–ï¼‰
WEALTH_NODE_DISPLAY = {
    "__start__": {"name": "ç”¨æˆ·è¾“å…¥", "type": "input", "icon": "ğŸ“", "description": "ç”¨æˆ·å’¨è¯¢ä¸ç”»åƒ"},
    "__end__": {"name": "å“åº”è¾“å‡º", "type": "output", "icon": "ğŸ“¢", "description": "æŠ•é¡¾å»ºè®®"},
    "assess": {"name": "åè°ƒè¯„ä¼°", "type": "llm", "icon": "ğŸ¯", "description": "è¯„ä¼°æŸ¥è¯¢ç±»å‹ä¸å¤„ç†æ¨¡å¼"},
    "reactive": {"name": "å³æ—¶å“åº”", "type": "llm", "icon": "âš¡", "description": "å¿«é€Ÿå›ç­”ç®€å•å’¨è¯¢"},
    "collect_data": {"name": "æ•°æ®æ”¶é›†", "type": "tool", "icon": "ğŸ“Š", "description": "æ”¶é›†å¸‚åœºä¸è´¢åŠ¡æ•°æ®"},
    "analyze": {"name": "åˆ†æå¼•æ“", "type": "llm", "icon": "ğŸ”", "description": "æŠ•èµ„ä¸ç»„åˆåˆ†æ"},
    "recommend": {"name": "å»ºè®®ç”Ÿæˆ", "type": "llm", "icon": "ğŸ’¡", "description": "ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®"},
    "respond": {"name": "æœ€ç»ˆå“åº”", "type": "process", "icon": "ğŸ“¤", "description": "è¾“å‡ºæŠ•é¡¾å»ºè®®"},
}


def create_wealth_advisor_workflow():
    """åˆ›å»ºè´¢å¯Œé¡¾é—®æ··åˆæ™ºèƒ½ä½“å·¥ä½œæµ"""
    workflow = StateGraph(WealthAdvisorState)
    workflow.add_node("assess", _assess_query)
    workflow.add_node("reactive", _reactive_processing)
    workflow.add_node("collect_data", _collect_data)
    workflow.add_node("analyze", _analyze_data)
    workflow.add_node("recommend", _generate_recommendations)
    workflow.add_node("respond", _respond)
    workflow.set_entry_point("assess")
    workflow.add_conditional_edges(
        "assess",
        lambda x: "reactive" if x.get("processing_mode") == "reactive" else "collect_data",
        {"reactive": "reactive", "collect_data": "collect_data"},
    )
    workflow.add_edge("reactive", "respond")
    workflow.add_edge("collect_data", "analyze")
    workflow.add_edge("analyze", "recommend")
    workflow.add_edge("recommend", "respond")
    workflow.add_edge("respond", END)
    return workflow.compile()


def get_wealth_advisor_graph_schema() -> dict:
    """è¿”å›è´¢å¯Œé¡¾é—®æ™ºèƒ½ä½“ LangGraph å›¾è°±ä¿¡æ¯ï¼Œä¾›å‰ç«¯å±•ç¤ºï¼ˆnodes + edgesï¼‰ã€‚"""
    graph = create_wealth_advisor_workflow()
    schema = graph_to_schema(graph, node_display=WEALTH_NODE_DISPLAY)
    schema.setdefault("executionOrder", [])
    return schema


def run_wealth_advisor_and_collect_steps(input_state: dict) -> dict:
    """
    æ‰§è¡Œè´¢å¯Œé¡¾é—®æ™ºèƒ½ä½“å¹¶æ”¶é›†æ¯æ­¥ä¿¡æ¯ï¼Œä¾›å‰ç«¯æŒ‰æ‰§è¡Œé¡ºåºä¸èŠ‚å¥å±•ç¤ºã€‚
    è¿”å›ï¼šgraphDataï¼ˆnodes, edges, executionOrderï¼‰ã€stepsã€finalStateã€executionOrderã€‚
    """
    graph = create_wealth_advisor_workflow()
    schema = graph_to_schema(graph, node_display=WEALTH_NODE_DISPLAY)
    run_result = run_graph_stream_and_collect(graph, input_state)
    return {
        "graphData": {
            "nodes": schema["nodes"],
            "edges": schema["edges"],
            "executionOrder": run_result["executionOrder"],
        },
        "steps": run_result["steps"],
        "finalState": run_result["finalState"],
        "executionOrder": run_result["executionOrder"],
    }
