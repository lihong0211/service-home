"""
æ·±æ€ç†Ÿè™‘å‹æ™ºèƒ½ä½“ - æ™ºèƒ½æŠ•ç ”åŠ©æ‰‹

åŸºäº LangGraph å®ç°çš„æ·±æ€ç†Ÿè™‘å‹æ™ºèƒ½ä½“ï¼Œé€‚ç”¨äºæŠ•èµ„ç ”ç©¶åœºæ™¯ï¼Œèƒ½å¤Ÿæ•´åˆæ•°æ®ï¼Œ
è¿›è¡Œå¤šæ­¥éª¤åˆ†æå’Œæ¨ç†ï¼Œç”ŸæˆæŠ•èµ„è§‚ç‚¹å’Œç ”ç©¶æŠ¥å‘Šã€‚

æ ¸å¿ƒæµç¨‹ï¼š
1. æ„ŸçŸ¥ï¼šæ”¶é›†å¸‚åœºæ•°æ®å’Œä¿¡æ¯
2. å»ºæ¨¡ï¼šæ„å»ºå†…éƒ¨ä¸–ç•Œæ¨¡å‹ï¼Œç†è§£å¸‚åœºçŠ¶æ€
3. æ¨ç†ï¼šç”Ÿæˆå¤šä¸ªå€™é€‰åˆ†ææ–¹æ¡ˆå¹¶æ¨¡æ‹Ÿç»“æœ
4. å†³ç­–ï¼šé€‰æ‹©æœ€ä¼˜æŠ•èµ„è§‚ç‚¹å¹¶å½¢æˆæŠ¥å‘Š
5. æŠ¥å‘Šï¼šç”Ÿæˆå®Œæ•´ç ”ç©¶æŠ¥å‘Š
"""

import json
import os
from typing import Any, Dict, List, Literal, Optional, TypedDict

from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph

from service.ai.langchain import graph_to_schema, run_graph_stream_and_collect

_API_KEY = os.environ.get("DASHSCOPE_API_KEY")
_LLM = ChatOpenAI(
    model="qwen-turbo",
    openai_api_key=_API_KEY,
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
    max_tokens=2000,
)


class ResearchAgentState(TypedDict):
    research_topic: str
    industry_focus: str
    time_horizon: str
    perception_data: Optional[Dict[str, Any]]
    world_model: Optional[Dict[str, Any]]
    reasoning_plans: Optional[List[Dict[str, Any]]]
    selected_plan: Optional[Dict[str, Any]]
    final_report: Optional[str]
    current_phase: Optional[str]
    error: Optional[str]


_PERCEPTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ•èµ„ç ”ç©¶åˆ†æå¸ˆï¼Œè¯·æ”¶é›†å’Œæ•´ç†å…³äºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜çš„å¸‚åœºæ•°æ®å’Œä¿¡æ¯ï¼š

ç ”ç©¶ä¸»é¢˜: {research_topic}
è¡Œä¸šç„¦ç‚¹: {industry_focus}
æ—¶é—´èŒƒå›´: {time_horizon}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œå¸‚åœºæ„ŸçŸ¥ï¼š
1. å¸‚åœºæ¦‚å†µå’Œæœ€æ–°åŠ¨æ€
2. å…³é”®ç»æµå’Œå¸‚åœºæŒ‡æ ‡
3. è¿‘æœŸé‡è¦æ–°é—»ï¼ˆè‡³å°‘3æ¡ï¼‰
4. è¡Œä¸šè¶‹åŠ¿åˆ†æï¼ˆè‡³å°‘é’ˆå¯¹3ä¸ªç»†åˆ†é¢†åŸŸï¼‰

æ ¹æ®ä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œç»éªŒï¼Œæä¾›å°½å¯èƒ½è¯¦ç»†å’Œå‡†ç¡®çš„ä¿¡æ¯ã€‚

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- market_overview: å­—ç¬¦ä¸²ï¼Œå¸‚åœºæ¦‚å†µ
- key_indicators: å¯¹è±¡ï¼Œé”®ä¸ºæŒ‡æ ‡åç§°ï¼Œå€¼ä¸ºæŒ‡æ ‡å€¼å’Œç®€è¦è§£é‡Š
- recent_news: æ•°ç»„ï¼Œæ¯é¡¹ä¸ºä¸€æ¡é‡è¦æ–°é—»
- industry_trends: å¯¹è±¡ï¼Œé”®ä¸ºç»†åˆ†é¢†åŸŸï¼Œå€¼ä¸ºè¶‹åŠ¿åˆ†æ
"""

_MODELING_PROMPT = """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æŠ•èµ„ç­–ç•¥å¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹å¸‚åœºæ•°æ®å’Œä¿¡æ¯ï¼Œæ„å»ºå¸‚åœºå†…éƒ¨æ¨¡å‹ï¼Œè¿›è¡Œæ·±åº¦åˆ†æï¼š

ç ”ç©¶ä¸»é¢˜: {research_topic}
è¡Œä¸šç„¦ç‚¹: {industry_focus}
æ—¶é—´èŒƒå›´: {time_horizon}

å¸‚åœºæ•°æ®å’Œä¿¡æ¯:
{perception_data}

è¯·æ„å»ºä¸€ä¸ªå…¨é¢çš„å¸‚åœºå†…éƒ¨æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
1. å½“å‰å¸‚åœºçŠ¶æ€è¯„ä¼°
2. ç»æµå‘¨æœŸåˆ¤æ–­
3. ä¸»è¦é£é™©å› ç´ ï¼ˆè‡³å°‘3ä¸ªï¼‰
4. æ½œåœ¨æœºä¼šé¢†åŸŸï¼ˆè‡³å°‘3ä¸ªï¼‰
5. å¸‚åœºæƒ…ç»ªåˆ†æ

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- market_state: å­—ç¬¦ä¸²ï¼Œå¸‚åœºçŠ¶æ€
- economic_cycle: å­—ç¬¦ä¸²ï¼Œç»æµå‘¨æœŸ
- risk_factors: æ•°ç»„ï¼Œé£é™©å› ç´ åˆ—è¡¨
- opportunity_areas: æ•°ç»„ï¼Œæœºä¼šé¢†åŸŸåˆ—è¡¨
- market_sentiment: å­—ç¬¦ä¸²ï¼Œå¸‚åœºæƒ…ç»ª
"""

_REASONING_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæˆ˜ç•¥æŠ•èµ„é¡¾é—®ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å¸‚åœºæ¨¡å‹ï¼Œç”Ÿæˆ3ä¸ªä¸åŒçš„æŠ•èµ„åˆ†ææ–¹æ¡ˆï¼š

ç ”ç©¶ä¸»é¢˜: {research_topic}
è¡Œä¸šç„¦ç‚¹: {industry_focus}
æ—¶é—´èŒƒå›´: {time_horizon}

å¸‚åœºå†…éƒ¨æ¨¡å‹:
{world_model}

è¯·ä¸ºæ¯ä¸ªæ–¹æ¡ˆæä¾›ï¼šplan_id, hypothesis, analysis_approach, expected_outcome, confidence_level, pros, consã€‚
è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¾“å‡ºã€‚
"""

_DECISION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæŠ•èµ„å†³ç­–å§”å‘˜ä¼šä¸»å¸­ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹å€™é€‰åˆ†ææ–¹æ¡ˆï¼Œé€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆå¹¶å½¢æˆæŠ•èµ„å†³ç­–ã€‚

ç ”ç©¶ä¸»é¢˜: {research_topic}
è¡Œä¸šç„¦ç‚¹: {industry_focus}
æ—¶é—´èŒƒå›´: {time_horizon}

å¸‚åœºå†…éƒ¨æ¨¡å‹:
{world_model}

å€™é€‰åˆ†ææ–¹æ¡ˆ:
{reasoning_plans}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- selected_plan_id: å­—ç¬¦ä¸²
- investment_thesis: å­—ç¬¦ä¸²
- supporting_evidence: æ•°ç»„
- risk_assessment: å­—ç¬¦ä¸²
- recommendation: å­—ç¬¦ä¸²
- timeframe: å­—ç¬¦ä¸²
"""

_REPORT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ•èµ„ç ”ç©¶æŠ¥å‘Šæ’°å†™äººï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä»½å®Œæ•´çš„æŠ•èµ„ç ”ç©¶æŠ¥å‘Šï¼š

ç ”ç©¶ä¸»é¢˜: {research_topic}
è¡Œä¸šç„¦ç‚¹: {industry_focus}
æ—¶é—´èŒƒå›´: {time_horizon}

å¸‚åœºæ•°æ®å’Œä¿¡æ¯:
{perception_data}

å¸‚åœºå†…éƒ¨æ¨¡å‹:
{world_model}

é€‰å®šçš„æŠ•èµ„å†³ç­–:
{selected_plan}

è¯·ç”Ÿæˆä¸€ä»½ç»“æ„å®Œæ•´ã€é€»è¾‘æ¸…æ™°çš„æŠ•ç ”æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼šæŠ¥å‘Šæ ‡é¢˜å’Œæ‘˜è¦ã€å¸‚åœºå’Œè¡Œä¸šèƒŒæ™¯ã€æ ¸å¿ƒæŠ•èµ„è§‚ç‚¹ã€è¯¦ç»†åˆ†æè®ºè¯ã€é£é™©å› ç´ ã€æŠ•èµ„å»ºè®®ã€æ—¶é—´æ¡†æ¶å’Œé¢„æœŸå›æŠ¥ã€‚
"""


def _research_perception(state: ResearchAgentState) -> ResearchAgentState:
    try:
        chain = ChatPromptTemplate.from_template(_PERCEPTION_PROMPT) | _LLM | JsonOutputParser()
        result = chain.invoke({
            "research_topic": state["research_topic"],
            "industry_focus": state["industry_focus"],
            "time_horizon": state["time_horizon"],
        })
        return {**state, "perception_data": result, "current_phase": "modeling"}
    except Exception as e:
        return {**state, "error": str(e), "current_phase": "perception"}


def _research_modeling(state: ResearchAgentState) -> ResearchAgentState:
    if not state.get("perception_data"):
        return {**state, "error": "å»ºæ¨¡é˜¶æ®µç¼ºå°‘æ„ŸçŸ¥æ•°æ®", "current_phase": "perception"}
    try:
        chain = ChatPromptTemplate.from_template(_MODELING_PROMPT) | _LLM | JsonOutputParser()
        result = chain.invoke({
            "research_topic": state["research_topic"],
            "industry_focus": state["industry_focus"],
            "time_horizon": state["time_horizon"],
            "perception_data": json.dumps(state["perception_data"], ensure_ascii=False, indent=2),
        })
        return {**state, "world_model": result, "current_phase": "reasoning"}
    except Exception as e:
        return {**state, "error": str(e), "current_phase": "modeling"}


def _research_reasoning(state: ResearchAgentState) -> ResearchAgentState:
    if not state.get("world_model"):
        return {**state, "error": "æ¨ç†é˜¶æ®µç¼ºå°‘ä¸–ç•Œæ¨¡å‹", "current_phase": "modeling"}
    try:
        chain = ChatPromptTemplate.from_template(_REASONING_PROMPT) | _LLM | JsonOutputParser()
        result = chain.invoke({
            "research_topic": state["research_topic"],
            "industry_focus": state["industry_focus"],
            "time_horizon": state["time_horizon"],
            "world_model": json.dumps(state["world_model"], ensure_ascii=False, indent=2),
        })
        return {**state, "reasoning_plans": result, "current_phase": "decision"}
    except Exception as e:
        return {**state, "error": str(e), "current_phase": "reasoning"}


def _research_decision(state: ResearchAgentState) -> ResearchAgentState:
    if not state.get("reasoning_plans"):
        return {**state, "error": "å†³ç­–é˜¶æ®µç¼ºå°‘å€™é€‰æ–¹æ¡ˆ", "current_phase": "reasoning"}
    try:
        chain = ChatPromptTemplate.from_template(_DECISION_PROMPT) | _LLM | JsonOutputParser()
        result = chain.invoke({
            "research_topic": state["research_topic"],
            "industry_focus": state["industry_focus"],
            "time_horizon": state["time_horizon"],
            "world_model": json.dumps(state["world_model"], ensure_ascii=False, indent=2),
            "reasoning_plans": json.dumps(state["reasoning_plans"], ensure_ascii=False, indent=2),
        })
        return {**state, "selected_plan": result, "current_phase": "report"}
    except Exception as e:
        return {**state, "error": str(e), "current_phase": "decision"}


def _research_report(state: ResearchAgentState) -> ResearchAgentState:
    if not state.get("selected_plan"):
        return {**state, "error": "æŠ¥å‘Šé˜¶æ®µç¼ºå°‘é€‰å®šæ–¹æ¡ˆ", "current_phase": "decision"}
    try:
        chain = ChatPromptTemplate.from_template(_REPORT_PROMPT) | _LLM | StrOutputParser()
        result = chain.invoke({
            "research_topic": state["research_topic"],
            "industry_focus": state["industry_focus"],
            "time_horizon": state["time_horizon"],
            "perception_data": json.dumps(state["perception_data"], ensure_ascii=False, indent=2),
            "world_model": json.dumps(state["world_model"], ensure_ascii=False, indent=2),
            "selected_plan": json.dumps(state["selected_plan"], ensure_ascii=False, indent=2),
        })
        return {**state, "final_report": result, "current_phase": "completed"}
    except Exception as e:
        return {**state, "error": str(e), "current_phase": "report"}


# èŠ‚ç‚¹ id -> å‰ç«¯å±•ç¤ºï¼ˆä¾› LangGraph å›¾è°±å¯è§†åŒ–ï¼‰
RESEARCH_NODE_DISPLAY = {
    "__start__": {"name": "ç”¨æˆ·è¾“å…¥", "type": "input", "icon": "ğŸ“", "description": "ç ”ç©¶ä¸»é¢˜ä¸å‚æ•°"},
    "__end__": {"name": "æŠ¥å‘Šè¾“å‡º", "type": "output", "icon": "ğŸ“¢", "description": "æŠ•ç ”æŠ¥å‘Š"},
    "perception": {"name": "æ„ŸçŸ¥", "type": "llm", "icon": "ğŸ”", "description": "æ”¶é›†å¸‚åœºæ•°æ®ä¸ä¿¡æ¯"},
    "modeling": {"name": "å»ºæ¨¡", "type": "llm", "icon": "ğŸ“", "description": "æ„å»ºå¸‚åœºå†…éƒ¨æ¨¡å‹"},
    "reasoning": {"name": "æ¨ç†", "type": "llm", "icon": "ğŸ¤”", "description": "ç”Ÿæˆå€™é€‰åˆ†ææ–¹æ¡ˆ"},
    "decision": {"name": "å†³ç­–", "type": "condition", "icon": "ğŸ¯", "description": "é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ"},
    "report": {"name": "æŠ¥å‘Š", "type": "llm", "icon": "ğŸ“„", "description": "ç”ŸæˆæŠ•ç ”æŠ¥å‘Š"},
}


def create_research_agent_workflow():
    """åˆ›å»ºæ·±æ€ç†Ÿè™‘å‹ç ”ç©¶æ™ºèƒ½ä½“å·¥ä½œæµå›¾"""
    workflow = StateGraph(ResearchAgentState)
    workflow.add_node("perception", _research_perception)
    workflow.add_node("modeling", _research_modeling)
    workflow.add_node("reasoning", _research_reasoning)
    workflow.add_node("decision", _research_decision)
    workflow.add_node("report", _research_report)
    workflow.set_entry_point("perception")
    workflow.add_edge("perception", "modeling")
    workflow.add_edge("modeling", "reasoning")
    workflow.add_edge("reasoning", "decision")
    workflow.add_edge("decision", "report")
    workflow.add_edge("report", END)
    return workflow.compile()


def get_research_agent_graph_schema() -> dict:
    """è¿”å›ç ”ç©¶æ™ºèƒ½ä½“ LangGraph å›¾è°±ä¿¡æ¯ï¼Œä¾›å‰ç«¯å±•ç¤ºï¼ˆnodes + edgesï¼‰ã€‚"""
    graph = create_research_agent_workflow()
    schema = graph_to_schema(graph, node_display=RESEARCH_NODE_DISPLAY)
    schema.setdefault("executionOrder", [])
    return schema


def run_research_agent_and_collect_steps(input_state: dict) -> dict:
    """
    æ‰§è¡Œç ”ç©¶æ™ºèƒ½ä½“å¹¶æ”¶é›†æ¯æ­¥ä¿¡æ¯ï¼Œä¾›å‰ç«¯æŒ‰æ‰§è¡Œé¡ºåºä¸èŠ‚å¥å±•ç¤ºã€‚
    è¿”å›ï¼šgraphDataï¼ˆnodes, edges, executionOrderï¼‰ã€stepsã€finalStateã€executionOrderã€‚
    """
    graph = create_research_agent_workflow()
    schema = graph_to_schema(graph, node_display=RESEARCH_NODE_DISPLAY)
    run_result = run_graph_stream_and_collect(graph, input_state)
    return {
        "graphData": {
            "nodes": schema["nodes"],
            "edges": schema["edges"],
            "executionOrder": run_result["executionOrder"],
        },
        "steps": run_result["steps"],
        "finalState": run_result["finalState"],
        "executionOrder": run_result["executionOrder"],
    }
